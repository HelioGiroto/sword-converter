#+OPTIONS: num:nil

* Notes

** [2018-01-11 Thu 03:57]  The SWORD project code base is a mostly undocumented mess

Maybe I shouldn't criticize too harshly, but man, what a mess.  I mean, the [[https://www.crosswire.org/wiki/DevTools:SWORD][developer tools wiki]] (which is not kept in sync with the code) actually says:

#+BEGIN_QUOTE
The API primer is quite old and has not been updated for a long time.If in doubt, consult the API Doxygen documentation (see below) and look at code samples within the source tree under /examples
#+END_QUOTE

But that's not the worst part.  The worst part is what they say about the [[https://www.crosswire.org/wiki/File_Formats][file formats]]:

#+BEGIN_QUOTE
Other than the source code for the SWORD API, there is no documentation for the file format of a SWORD module. The intention is that the SWORD API (or the JSword implementation) is used directly or via other language bindings.

Our module file format is proprietary in the sense that we see no need to document it and certainly no need to stick to it. We change it when we need to. We therefore do not encourage direct interaction with it, but firmly recommend use of the API (either C++ or Java). This is the place where we seek stability and consistency.
#+END_QUOTE

Good grief.

** [2018-01-12 Fri 11:10]  Working C++ converter!

Well, with some stumbling around I was able to modify the =lookup= example utility to output JSON.  It converts the entire ESV module in about 4 seconds!  That's in contrast to the multithreaded Python script that calls Diatheke for each individual verse, which takes over 20 minutes.

[2018-01-12 Fri 11:46]  Well, that works in the 1.8.0 source directory, but the executable it produced only works when called from a shell script in the source directory that is generated by the build process.  I guess if I installed the whole package (libsword and all), the resulting executable would work from elsewhere.

So I tried to make a version for 1.6.2 (the version in Ubuntu Trusty), and it almost works correctly, but the verse text has =[]= in random places.  I don't know if it's because of something I did wrong or changes in the SWORD API.  I had to add some code I found on StackOverflow to trim strings, and I had to convert some of the old API output to =std::string=.

So the problems now are:

+  The 1.8.0 version only runs from within the source directory.
+  The 1.6.2 version has garbage in the output.

If I could build the 1.8.0 version statically, that would probably solve it, but I've no idea how to just build one executable in a project statically.  Might be a simple switch in a command, or it might be much more complex.

I might be able to fix the output in the 1.6.2 version, but since I don't understand exactly what's going on with the buffers and strings and such, I'm not sure what to do.

[2018-01-12 Fri 12:05]  I found [[https://stackoverflow.com/a/15475134][this StackOverflow answer]], and I modified the Makefile in both versions (in the =examples/cmdline= directory) to add those flags to =CXXCOMPILE= and =CXXLINK=, and it made the =examples/cmdline/lookup= file a binary instead of the shell script, but apparently only the =libsword= stuff was statically linked, because =ldd= shows that it's still dynamically linked to a bunch of general shared libraries like =libc=.  But the binary does work when I copy it out of the source directory, so I guess it will do.  This lets me statically link the 1.8.0 version of =libsword=, so I can use it with 1.6.2 still installed on the system.

** [2018-01-13 Sat 04:35]  Time for database?

Now that I have a decent JSON version, maybe it's time to think about SQLite database schemas.  There are two basic possibilities that I can think of:

1.  One table for verses, mapping book IDs to a book table which contains book names and abbreviations.
2.  One table for verses, with book names as strings.

The second one might actually be better because of its simplicity, and with the proper indexing, it should perform fine, AFAIK.

[2018-01-13 Sat 06:33]  It works!  The new =json-to-sqlite.py= script reads the flat-style JSON file (one object per verse, not hierarchical), makes a new SQLite database, and inserts the verses.  I used a Python iterator to make the code fairly simple.

I also wrote a Python script yesterday, =json-verses-to-bible.py=, that converts a flat JSON list of objects to a hierarchical object.  Seems like that could be handy, but it's easier to insert the flat version into the database, and that's what the C++ program makes.

I guess now I'll write a Python script to query the database...

** [2018-01-13 Sat 07:50]  Python is faster than the C++ SWORD API

This is amazing:

#+BEGIN_EXAMPLE
$ time ./query.py query esv.json "Genesis" >/dev/null
0.14user 0.07system 0:00.21elapsed 98%CPU (0avgtext+0avgdata 40204maxresident)k
0inputs+0outputs (0major+10468minor)pagefaults 0swaps

$ time diatheke -b ESV -k "Genesis" >/dev/null
0.37user 0.02system 0:00.41elapsed 98%CPU (0avgtext+0avgdata 23328maxresident)k
0inputs+0outputs (0major+3792minor)pagefaults 0swaps
#+END_EXAMPLE

Printing the entire book of Genesis, parsed from a JSON file containing the entire ESV (minus all the metadata the SWORD formats can contain) is about twice as fast as printing Genesis using Diatheke, which is written in C++ and uses the SWORD C++ libraries.

Now imagine if we /did/ want to store all that metadata, if we stored it separately from the plain-text of the translation.  Whether SQLite or JSON, imagine how much faster it would be!

Oh, and querying the SQLite database is even faster still:

#+BEGIN_EXAMPLE
  $ time ./query.py query esv.sqlite "Genesis" >/dev/null
  0.06user 0.01system 0:00.09elapsed 77%CPU (0avgtext+0avgdata 13172maxresident)k
  8inputs+0outputs (0major+2215minor)pagefaults 0swaps
#+END_EXAMPLE

And I can easily change the output format too:

#+BEGIN_EXAMPLE
  $ time ./query.py query --output json esv.sqlite "Genesis" >/dev/null
  0.08user 0.02system 0:00.11elapsed 99%CPU (0avgtext+0avgdata 16776maxresident)k
  0inputs+0outputs (0major+3103minor)pagefaults 0swaps
#+END_EXAMPLE

Which effectively gives me two-way conversion between formats.

This makes everything so much easier.  Now I should be able to easily access the Bible in Emacs, too.

[2018-01-13 Sat 09:17]  Added full-text search with SQLite's FTS4 engine.  It's so much faster than Diatheke:

#+BEGIN_EXAMPLE
  $ time diatheke -b ESV -s multiword -k jesus >/dev/null
  1.40user 0.16system 0:01.58elapsed 99%CPU (0avgtext+0avgdata 27508maxresident)k
  0inputs+0outputs (0major+5097minor)pagefaults 0swaps

  $ time ./query.py search esv.sqlite "Jesus" >/dev/null
  0.06user 0.00system 0:00.07elapsed 97%CPU (0avgtext+0avgdata 13424maxresident)k
  0inputs+0outputs (0major+2276minor)pagefaults 0swaps
#+END_EXAMPLE

And note that Diatheke /only returns the references, not the actual text of the passages/, while this Python script returns the full text of every matching passage.  In fact, even doing full-text search on the JSON file in Python is much faster than Diatheke:

#+BEGIN_EXAMPLE
  $ time ./query.py search esv.json "Jesus" >/dev/null
  0.12user 0.08system 0:00.21elapsed 99%CPU (0avgtext+0avgdata 40056maxresident)k
  0inputs+0outputs (0major+7608minor)pagefaults 0swaps
#+END_EXAMPLE

** [2018-01-13 Sat 12:34]  Ranges next?
:PROPERTIES:
:ID:       94ff98fd-ff8b-480f-9109-f007c750bc9a
:END:

Another thing I'd like to do is support range queries.  To do that properly, between books, requires that we know what order the books are in.  Of course, that's very easy with SQL:

#+BEGIN_SRC sql
  SELECT DISTINCT book FROM verses
#+END_SRC

But that's the simple part.  The in-between across books is easy (just select all verses in those books), but the beginning and end of the range requires going up to the end of each chapter/verse in the range.  (I feel like this would be a lot easier to do in Lisp.)

** [2018-01-13 Sat 15:59]  Experimenting with compression

Been experimenting with compressing the SQLite database.  There is no built-in compression for SQLite except in the proprietary version.  But it [[https://www.sqlite.org/fts3.html#the_compress_and_uncompress_options][does]] support compression through SQL scalar functions registered with =sqlite3_create_function()=, which I discovered through a Google search that brought up [[http://www.perlmonks.org/?node_id=1064966][this PerlMonks question]].  (It is in the SQLite manual, but I didn't realize that functions from the calling DBI's language can be used, so e.g. Python functions can be registered.)

There are a few different approaches that can be used:

*** Uncompressed, non-FTS, =WITHOUT ROWID=

Without FTS, you don't get the FTS index, but doing FTS with plain =LIKE= (instead of the FTS-specific =MATCH=) is actually quite fast.

#+BEGIN_SRC sql
  CREATE TABLE verses (
  book TEXT,
  chapter INTEGER,
  verse INTEGER,
  text BLOB,
  PRIMARY KEY(book, chapter, verse))
  WITHOUT ROWID
#+END_SRC

Note that using the =sqlite3= utility is faster than the Python script, of course.  We can't use =sqlite3= on the compressed versions anyway, but it's still impressive that the uncompressed, non-FTS one does the raw SQLite search in 0.04 seconds:

#+BEGIN_EXAMPLE
  $ time sqlite3 esv.without-rowid.sqlite "SELECT * FROM verses WHERE text LIKE '%Moses%'" >/dev/null
  0.02user 0.01system 0:00.04elapsed 90%CPU (0avgtext+0avgdata 5288maxresident)k
  0inputs+0outputs (0major+811minor)pagefaults 0swaps
#+END_EXAMPLE

The Python script is still quick:

#+BEGIN_EXAMPLE
  $ time ./query.py search esv.without-rowid.sqlite Moses >/dev/null
  0.12user 0.00system 0:00.13elapsed 98%CPU (0avgtext+0avgdata 17712maxresident)k
  0inputs+0outputs (0major+3040minor)pagefaults 0swaps
#+END_EXAMPLE

Selection is done normally:

#+BEGIN_SRC sql
  -- NOTE: The keyword for the LIKE must be surrounded with % signs in Python.
  SELECT * FROM verses WHERE text LIKE ?
#+END_SRC

0.13 seconds for searching the whole ESV for any verse containing "Moses" is really fast--much faster than Diatheke, which takes 1.24 seconds (without returning the actual text of the matches)!  So this is actually quite a viable solution.  However, as we'll see, the FTS4 tables do search much faster (0.00 seconds for a =MATCH "Moses"=).

The file is smaller than the FTS versions: 6.4 MB.

*** Uncompressed, FTS

Without compression, it's all very simple:

#+BEGIN_SRC sql
  CREATE VIRTUAL TABLE verses USING fts4 (
  book TEXT,
  chapter INTEGER,
  verse INTEGER,
  text TEXT)
#+END_SRC

#+BEGIN_SRC sql
  SELECT * FROM verses WHERE text MATCH ?
#+END_SRC

And it's quick:

#+BEGIN_EXAMPLE
  $ time sqlite3 esv.fts.sqlite "SELECT * FROM verses WHERE text MATCH 'Moses'" >/dev/null
  0.00user 0.00system 0:00.00elapsed 66%CPU (0avgtext+0avgdata 3664maxresident)k
  0inputs+0outputs (0major+333minor)pagefaults 0swaps
#+END_EXAMPLE

Yes, actually 0.00 seconds (well, less than =time= can measure, anyway, and with =sqlite3=, not the Python script), and that's returning the full text of every match.  The Python script is also pretty fast, at 0.10 seconds.  The size is the largest: 9.3 MB.

*** Compressed, non-FTS, =WITHOUT ROWID=

Without FTS, it's simple to insert data with certain fields compressed using Python's Zlib or LZMA functions.  This means you don't get the FTS index, but doing FTS with plain =LIKE= (instead of the FTS-specific =MATCH=) is still reasonably fast.

Selection is done with manual decompression:

#+BEGIN_SRC python
  def decompress(data):
      return zlib.decompress(data).decode('utf-8')

  con = sqlite3.connect(filename)
  con.create_function("decompress", 1, decompress)

  # Note that when using the Python sqlite3.Row row_factory, the field
  # names used include the decompression function, like:
  row['decompress(text)']
#+END_SRC

#+BEGIN_SRC sql
  SELECT book, chapter, verse, decompress(text) FROM verses WHERE book LIKE ? AND chapter=? AND verse=?
#+END_SRC

Searching this way is slower, at 0.33 seconds:

#+BEGIN_EXAMPLE
  $ time ./query.py search esv.compressed.zlib.without-rowid.sqlite Moses >/dev/null
  0.32user 0.00system 0:00.33elapsed 98%CPU (0avgtext+0avgdata 17732maxresident)k
  0inputs+0outputs (0major+3053minor)pagefaults 0swaps
#+END_EXAMPLE

Decompressing every verse to match against is is slower, but still much faster than Diatheke's 1.24 seconds!

And this results in the smallest file: 3.8 MB compressed.

*** Compressed, pure FTS

Using FTS, the =compress= and =uncompress= options can be passed to the =CREATE TABLE= statement, specifying Python functions to handle compression and decompression (and the same decompression function must be used in the query script).  This reduced the size from 9.3 MB to about 8.1 MB.  I say "about" because I only got it to work by not storing the chapter and verse numbers.  I think it's a data-type problem, as mentioned in the manual:

#+BEGIN_QUOTE
When implementing the compress and uncompress functions it is important to pay attention to data types. Specifically, when a user reads a value from a compressed FTS table, the value returned by FTS is exactly the same as the value returned by the uncompress function, including the data type. If that data type is not the same as the data type of the original value as passed to the compress function (for example if the uncompress function is returning BLOB when compress was originally passed TEXT), then the users query may not function as expected.
#+END_QUOTE

I think it wasn't working because I was trying to store the numbers as integers, and the compression/decompression functions I made were for strings.  I could probably fix that with some more fiddling.

For creation and insertion:

#+BEGIN_SRC sql
  CREATE VIRTUAL TABLE verses USING fts4 (
  book, text,
  compress=compress, uncompress=decompress)
#+END_SRC

#+BEGIN_SRC python
  con = sqlite3.connect(db_filename)

  def compress(s):
      return zlib.compress(s.encode('utf-8'), 9)

  con.create_function("compress", 1, compress)

  # Not sure if this actually has to be specified for the insertion
  def decompress(data):
      return zlib.decompress(data).decode('utf-8')

  con.create_function("decompress", 1, decompress)
#+END_SRC

Doing it this way doesn't require putting the decompression function in the queries:

#+BEGIN_SRC sql
  SELECT book, text FROM verses WHERE text MATCH ?
#+END_SRC

Or the field names for =row_factory= access:

#+BEGIN_SRC python
  con = sqlite3.connect(filename)

  def decompress(data):
      return zlib.decompress(data).decode('utf-8')

  con.create_function("decompress", 1, decompress)

  # ...Do the query, and...
  row['text']
#+END_SRC

And this is fast to search:

#+BEGIN_EXAMPLE
  $ time ./query.py search esv.compressed.zlib.pure-fts.sqlite Moses >/dev/null
  0.11user 0.00system 0:00.11elapsed 99%CPU (0avgtext+0avgdata 15888maxresident)k
  0inputs+0outputs (0major+2553minor)pagefaults 0swaps
#+END_EXAMPLE

The file size is 8.1 MB.

*** FTS with external content and manually compressed insertions

Using FTS with its =content== option stores the actual data in another table, and queries run against the FTS table that need data in the content table automatically fetch it.

#+BEGIN_SRC sql
  CREATE TABLE verses (
  book TEXT, chapter INTEGER, verse INTEGER, text BLOB,
  PRIMARY KEY(book, chapter, verse))

  CREATE VIRTUAL TABLE fts USING fts4 (
  book, chapter, verse, text,
  content="verses", uncompress=decompress)
#+END_SRC

Then insertion is done with manual compression:

#+BEGIN_SRC python
  def compress(s):
      return zlib.compress(s.encode('utf-8'), 9)

  def decompress(data):
      return zlib.decompress(data).decode('utf-8')

  con = sqlite3.connect(db_filename)
  con.create_function("compress", 1, compress)
#+END_SRC

#+BEGIN_SRC sql
  INSERT INTO verses(book, chapter, verse, text) VALUES (?, ?, ?, compress(?))
#+END_SRC

And then the FTS database is rebuilt against the source table (which is easier than inserting into the index manually, or using triggers, and makes sense since we're only doing insertion in one batch):

#+BEGIN_SRC sql
  INSERT INTO fts(fts) VALUES('rebuild')

  -- I think optimization still works, but I'm not sure:
  INSERT INTO fts(fts) VALUES('optimize')
#+END_SRC

Queries are done against the FTS table, with the decompressor specified again:

#+BEGIN_SRC sql
  SELECT book, chapter, verse, decompress(text) FROM fts WHERE text MATCH ?
#+END_SRC

#+BEGIN_SRC python
  def decompress(data):
      return zlib.decompress(data).decode('utf-8')

  con = sqlite3.connect(filename)
  con.create_function("decompress", 1, decompress)

  # ...Do query, and...
  row['decompress(text)']
#+END_SRC

This is pretty fast:

#+BEGIN_EXAMPLE
  $ time ./query.py search esv.compressed.zlib.external-content.sqlite Moses >/dev/null
  0.09user 0.02system 0:00.11elapsed 99%CPU (0avgtext+0avgdata 15944maxresident)k
  0inputs+0outputs (0major+2587minor)pagefaults 0swaps
#+END_EXAMPLE

However the database is only slightly smaller than the uncompressed FTS one, at 8.9 MB.  Who knows, maybe I did a step wrong somewhere, because since the non-FTS database shrank by about 2.6 MB when compressed, it seems like the FTS table using the compressed one as external-content shouldn't add about 2.2 MB.  But I just ran =optimize= and =VACUUM= on the =fts= table again, and it made no difference, so I don't know what else to do.

So for the compressed, FTS, external-content version it appears that it's less space-efficient than the compressed, pure-FTS version (I guess because it's storing the same data, plus extra data to correlate the index with the external content).  I guess this option makes more sense for "contentless" tables (and so I wonder in what cases external-content tables are useful...).

*** Lookup speed

IIRC, I was thinking that the pure-FTS table would be slower for key-based lookup (book, chapter, verse) since it [[https://www.sqlite.org/fts3.html#simple_fts_queries][can't be indexed on other columns]]:

#+BEGIN_QUOTE
FTS tables can be queried efficiently using SELECT statements of two different forms:

Query by rowid. If the WHERE clause of the SELECT statement contains a sub-clause of the form "rowid = ?", where ? is an SQL expression, FTS is able to retrieve the requested row directly using the equivalent of an SQLite INTEGER PRIMARY KEY index.

Full-text query. If the WHERE clause of the SELECT statement contains a sub-clause of the form "<column> MATCH ?", FTS is able to use the built-in full-text index to restrict the search to those documents that match the full-text query string specified as the right-hand operand of the MATCH clause.

If neither of these two query strategies can be used, all queries on FTS tables are implemented using a linear scan of the entire table. If the table contains large amounts of data, this may be an impractical approach (the first example on this page shows that a linear scan of 1.5 GB of data takes around 30 seconds using a modern PC).
#+END_QUOTE

And since I didn't get the compressed, pure-FTS one working with chapter and verse numbers, I can't test that one.  However, I can test the uncompressed ones, and the lookup speed doesn't seem bad:

#+BEGIN_EXAMPLE
  $ time sqlite3 esv.without-rowid.sqlite "SELECT * FROM verses WHERE book LIKE 'Genesis' AND chapter=1 AND verse=1" >/dev/null
  0.00user 0.00system 0:00.01elapsed 84%CPU (0avgtext+0avgdata 5400maxresident)k
  0inputs+0outputs (0major+812minor)pagefaults 0swaps

  $ time sqlite3 esv.fts.sqlite "SELECT * FROM verses WHERE book LIKE 'Genesis' AND chapter=1 AND verse=1" >/dev/null
  0.02user 0.00system 0:00.03elapsed 90%CPU (0avgtext+0avgdata 5572maxresident)k
  0inputs+0outputs (0major+811minor)pagefaults 0swaps
#+END_EXAMPLE

So it's 3 times slower to do lookups with the FTS one (which doesn't have indexes on book, chapter, or verse), but it's still 0.03 seconds, which is surely fast enough.  After all, the Python script is the bottleneck:

#+BEGIN_EXAMPLE
  $ time ./query.py lookup esv.fts.sqlite "Genesis 1:1" >/dev/null
  0.09user 0.00system 0:00.10elapsed 97%CPU (0avgtext+0avgdata 17396maxresident)k
  0inputs+0outputs (0major+2968minor)pagefaults 0swaps
#+END_EXAMPLE

*** Conclusion

Let's compare the speeds and sizes:

| Compression  | Search query                 | Speed | Size   |
|--------------+------------------------------+-------+--------|
| Uncompressed | LIKE (non-FTS)               |  0.13 | 6.4 MB |
| Uncompressed | MATCH (FTS)                  |  0.10 | 9.3 MB |
| Compressed   | LIKE (non-FTS)               |  0.33 | 3.8 MB |
| Compressed   | MATCH (FTS)                  |  0.11 | 8.1 MB |
| Compressed   | MATCH (FTS external-content) |  0.11 | 8.9 MB |

Sorted by speed:

| Compression  | Search query                 | Speed | Size   |
|--------------+------------------------------+-------+--------|
| Uncompressed | MATCH (FTS)                  |  0.10 | 9.3 MB |
| Compressed   | MATCH (FTS)                  |  0.11 | 8.1 MB |
| Compressed   | MATCH (FTS external-content) |  0.11 | 8.9 MB |
| Uncompressed | LIKE (non-FTS)               |  0.13 | 6.4 MB |
| Compressed   | LIKE (non-FTS)               |  0.33 | 3.8 MB |

By size:

| Compression  | Search query                 | Speed | Size   |
|--------------+------------------------------+-------+--------|
| Compressed   | LIKE (non-FTS)               |  0.33 | 3.8 MB |
| Uncompressed | LIKE (non-FTS)               |  0.13 | 6.4 MB |
| Compressed   | MATCH (FTS)                  |  0.11 | 8.1 MB |
| Compressed   | MATCH (FTS external-content) |  0.11 | 8.9 MB |
| Uncompressed | MATCH (FTS)                  |  0.10 | 9.3 MB |

For my personal use, there's no reason to do compression, and the book-chapter-verse lookup is fast enough with FTS, so I might as well use uncompressed FTS.

For general use, I'm not sure which is the best overall solution. The speed for the compressed, non-FTS version is much slower than the rest, but still much faster than Diatheke.  But on older, lower-powered systems (like old netbooks), that 0.33 seconds will be higher.  The size isn't important on-disk.  Over a network (if these "modules" were ever distributed), the size would be more important--but if it were really important, the JSON version of a module could be compressed with =xz= to a smaller size than the SWORD modules (and even =bzip2= or =gzip= on the JSON files makes them smaller than the SWORD modules).

Of course, the SWORD modules are storing more data than I am here, as I haven't converted any metadata, so it's not a fair comparison.  Storing all the metadata in JSON and the SQLite files would surely make them larger than the SWORD modules.  However, size is not nearly so important as it was when the SWORD project started, and the convenience and ease-of-use on the programming side would probably make that tradeoff worth it.  And the SQLite databases can be compressed as files for transport, anyway: the uncompressed, pure-FTS SQLite database file goes down to 2.5 MB with =xz -9=.  And of course the SQLite databases can be generated locally from a more compressible format. 

Another important thing to note: all of these tests were run on a single keyword query.  The FTS engine can easily do complex boolean matching (the syntax is very simple to use, like =Jesus Paul -apostle=), the non-FTS equivalent of which would probably be much slower and more difficult.  So that gives the FTS version a significant advantage for searching.

Well, all of that aside, I think it would be fairly simple to support the different kinds of databases in the scripts, because they can inspect the database and determine which kind of query to make and whether to use decompression.

* Design

** Query script

*** Features

**** TODO Ranges

+ [[id:94ff98fd-ff8b-480f-9109-f007c750bc9a][{2018-01-13 Sat 12:34}  Ranges next?]]

*** Docs

+  Python
     -  Core
          +  [[https://docs.python.org/3/library/re.html][6.2. re — Regular expression operations — Python 3.6.4 documentation]]
          +  [[https://docs.python.org/3/library/sqlite3.html][12.6. sqlite3 — DB-API 2.0 interface for SQLite databases — Python 3.6.4 documentation]]
     -  Packages
          +  [[https://pypi.python.org/pypi/blessings][blessings 1.6.1 : Python Package Index]]
          +  [[http://click.pocoo.org/5/][Click Documentation (5.0)]]
          +  [[https://pypi.python.org/pypi/colorama][colorama 0.3.9 : Python Package Index]]
+  SQLite
     -  [[https://www.sqlite.org/lang.html][Query Language Understood by SQLite]]
     -  [[https://www.sqlite.org/fts3.html][SQLite FTS3 and FTS4 Extensions]]

** SQLite Database

*** Features

**** [[https://en.wikipedia.org/wiki/Database_index#Clustered][Clustered index]] / [[https://www.sqlite.org/withoutrowid.html][WITHOUT ROWID]]

*Note:* This is incompatible with the FTS4 engine, so we're not using this after all.

Sounds like I should use this feature.

#+BEGIN_QUOTE
Clustering alters the data block into a certain distinct order to match the index, resulting in the row data being stored in order. Therefore, only one clustered index can be created on a given database table. Clustered indices can greatly increase overall speed of retrieval, but usually only where the data is accessed sequentially in the same or reverse order of the clustered index, or when a range of items is selected.

Since the physical records are in this sort order on disk, the next row item in the sequence is immediately before or after the last one, and so fewer data block reads are required. The primary feature of a clustered index is therefore the ordering of the physical data rows in accordance with the index blocks that point to them. Some databases separate the data and index blocks into separate files, others put two completely different data blocks within the same physical file(s).
#+END_QUOTE

I'm not sure if this means that I need to insert the verses in order (although I intend to, anyway).  But since the book-chapter-verse triplet won't sort in Bible-order (since book names aren't sorted alphabetically), I probably need to.  (I guess I could store book order in a separate table, but that is probably not worth the trouble.)

#+BEGIN_QUOTE
Thus, in some cases, a WITHOUT ROWID table can use about half the amount of disk space and can operate nearly twice as fast. Of course, in a real-world schema, there will typically be secondary indices and/or UNIQUE constraints, and the situation is more complicated. But even then, there can often be space and performance advantages to using WITHOUT ROWID on tables that have non-integer or composite PRIMARY KEYs.
#+END_QUOTE

Sounds like this is definitely what I should use.

#+BEGIN_QUOTE
The WITHOUT ROWID optimization is likely to be helpful for tables that have non-integer or composite (multi-column) PRIMARY KEYs and that do not store large strings or BLOBs.
#+END_QUOTE

I'm not sure what counts as a "large string," but I'm guessing that individual verses here aren't large enough to.

#+BEGIN_QUOTE
WITHOUT ROWID tables work best when individual rows are not too large. A good rule-of-thumb is that the average size of a single row in a WITHOUT ROWID table should be less than about 1/20th the size of a database page. That means that rows should not contain more than about 50 bytes each for a 1KiB page size or about 200 bytes each for 4KiB page size. WITHOUT ROWID tables will work (in the sense that they get the correct answer) for arbitrarily large rows - up to 2GB in size - but traditional rowid tables tend to work faster for large row sizes. This is because rowid tables are implemented as B*-Trees where all content is stored in the leaves of the tree, whereas WITHOUT ROWID tables are implemented using ordinary B-Trees with content stored on both leaves and intermediate nodes. Storing content in intermediate nodes mean that each intermediate node entry takes up more space on the page and thus reduces the fan-out, increasing the search cost.
#+END_QUOTE

For most verses that probably does apply (although I don't know what the page size will be, or if I should set it manually).  Most verses are probably less than 50 bytes, and probably few, if any, are 200 bytes (although I don't know if that should include the book name).

#+BEGIN_QUOTE
Note that except for a few corner-case differences detailed above, WITHOUT ROWID tables and rowid tables work the same. They both generate the same answers given the same SQL statements. So it is a simple matter to run experiments on an application, late in the development cycle, to test whether or not the use of WITHOUT ROWID tables will be helpful. A good strategy is to simply not worry about WITHOUT ROWID until near the end of product development, then go back and run tests to see if adding WITHOUT ROWID to tables with non-integer PRIMARY KEYs helps or hurts performance, and retaining the WITHOUT ROWID only in those cases where it helps.
#+END_QUOTE

Ok, so it's not a big deal, and I can easily change it later.

**** [[https://www.sqlite.org/datatype3.html#collation][Collation]]

#+BEGIN_QUOTE
When SQLite compares two strings, it uses a collating sequence or collating function (two words for the same thing) to determine which string is greater or if the two strings are equal. SQLite has three built-in collating functions: BINARY, NOCASE, and RTRIM.

BINARY - Compares string data using memcmp(), regardless of text encoding.
NOCASE - The same as binary, except the 26 upper case characters of ASCII are folded to their lower case equivalents before the comparison is performed. Note that only ASCII characters are case folded. SQLite does not attempt to do full UTF case folding due to the size of the tables required.
RTRIM - The same as binary, except that trailing space characters are ignored.
#+END_QUOTE

I guess I should probably use =NOCASE=, but for matching book names I will probably want something more advanced.

**** Primary key

*Note:* Also not relevant for FTS4.

#+BEGIN_QUOTE
Each table in SQLite may have at most one PRIMARY KEY. If the keywords PRIMARY KEY are added to a column definition, then the primary key for the table consists of that single column. Or, if a PRIMARY KEY clause is specified as a table-constraint, then the primary key of the table consists of the list of columns specified as part of the PRIMARY KEY clause. The PRIMARY KEY clause must contain only column names — the use of expressions in an indexed-column of a PRIMARY KEY is not supported. An error is raised if more than one PRIMARY KEY clause appears in a CREATE TABLE statement. The PRIMARY KEY is optional for ordinary tables but is required for WITHOUT ROWID tables.
#+END_QUOTE

I guess I should make it on the =book=, =chapter=, and =verse= columns.

**** UNDERWAY Full-text search
:LOGBOOK:
-  State "UNDERWAY"   from "DONE"       [2018-01-13 Sat 11:03]
:END:

Works great with SQLite's FTS4 engine.

***** TODO Query syntax
:LOGBOOK:
-  State "TODO"       from              [2018-01-13 Sat 11:03]
:END:

Note this from the FTS docs:

#+BEGIN_QUOTE
The FTS modules may be compiled to use one of two slightly different versions of the full-text query syntax, the "standard" query syntax and the "enhanced" query syntax. The basic term, term-prefix, phrase and NEAR queries described above are the same in both versions of the syntax. The way in which set operations are specified is slightly different. The following two sub-sections describe the part of the two query syntaxes that pertains to set operations. Refer to the description of how to compile fts for compilation notes.
#+END_QUOTE

And one of the key differences is:

#+BEGIN_QUOTE
FTS query set operations using the standard query syntax are similar, but not identical, to set operations with the enhanced query syntax. There are four differences, as follows:

1.  Only the implicit version of the AND operator is supported. Specifying the string "AND" as part of a standard query syntax query is interpreted as a term query for the set of documents containing the term "and".
#+END_QUOTE

Apparently my SQLite uses the standard syntax, because when I search for =Jesus AND Paul AND apostle AND called=, I only get one result:

#+BEGIN_EXAMPLE
  1 Corinthians 1:1: Paul, called by the will of God to be an apostle of Christ Jesus, and our brother Sosthenes,
#+END_EXAMPLE

While if I search for =Jesus Paul apostle called= I get:

#+BEGIN_EXAMPLE
  Romans 1:1: Paul, a servant of Christ Jesus, called to be an apostle, set apart for the gospel of God,
  I Corinthians 1:1: Paul, called by the will of God to be an apostle of Christ Jesus, and our brother Sosthenes,
#+END_EXAMPLE

So the =AND= is interpreted literally, and Ro 1:1 is not returned because it doesn't have =and= in it.

I can't control this, because it depends on how the local SQLite copy is compiled, so I'll have to document it in the readme.

**** Unique constraint

#+BEGIN_QUOTE
A UNIQUE constraint is similar to a PRIMARY KEY constraint, except that a single table may have any number of UNIQUE constraints. For each UNIQUE constraint on the table, each row must contain a unique combination of values in the columns identified by the UNIQUE constraint. For the purposes of UNIQUE constraints, NULL values are considered distinct from all other values, including other NULLs. As with PRIMARY KEYs, a UNIQUE table-constraint clause must contain only column names — the use of expressions in an indexed-column of a UNIQUE table-constraint is not supported.
#+END_QUOTE

So I do /not/ want a unique constraint on the =text= column, because even though it's unlikely that two verses in the Bible would be exactly the same, I don't know if it's actually the case, and it's theoretically possible.

*** Schema

**** Primary key

**** TODO =PRAGMA query_only=

I should probably enable this after creating the database.

**** Tables

***** Verses

#+BEGIN_SRC sql
  CREATE TABLE verses (
  book TEXT PRIMARY KEY,
  chapter INTEGER PRIMARY KEY,
  verse INTEGER PRIMARY KEY,
  text TEXT)
  WITHOUT ROWID;
#+END_SRC

FTS4 version:

#+BEGIN_SRC sql
  CREATE VIRTUAL TABLE verses USING fts4 (
  book TEXT,
  chapter INTEGER,
  verse INTEGER,
  text TEXT);
#+END_SRC

***** Book abbreviations

Might be a good idea to have this, but we'll probably still need some client-level heuristics to handle book-choosing.

***** Book numbers

Might be a good idea to have a table that maps book names to book numbers.  Across translations or (or...what is the right word?  codex?  anthology?  work?) types of Bibles, this might vary, but within a database, for a single translation, it would likely be useful.
