

* Notes

** [2018-01-11 Thu 03:57]  The SWORD project code base is a mostly undocumented mess

Maybe I shouldn't criticize too harshly, but man, what a mess.  I mean, the [[https://www.crosswire.org/wiki/DevTools:SWORD][developer tools wiki]] (which is not kept in sync with the code) actually says:

#+BEGIN_QUOTE
The API primer is quite old and has not been updated for a long time.If in doubt, consult the API Doxygen documentation (see below) and look at code samples within the source tree under /examples
#+END_QUOTE

But that's not the worst part.  The worst part is what they say about the [[https://www.crosswire.org/wiki/File_Formats][file formats]]:

#+BEGIN_QUOTE
Other than the source code for the SWORD API, there is no documentation for the file format of a SWORD module. The intention is that the SWORD API (or the JSword implementation) is used directly or via other language bindings.

Our module file format is proprietary in the sense that we see no need to document it and certainly no need to stick to it. We change it when we need to. We therefore do not encourage direct interaction with it, but firmly recommend use of the API (either C++ or Java). This is the place where we seek stability and consistency.
#+END_QUOTE

Good grief.

** [2018-01-12 Fri 11:10]  Working C++ converter!

Well, with some stumbling around I was able to modify the =lookup= example utility to output JSON.  It converts the entire ESV module in about 4 seconds!  That's in contrast to the multithreaded Python script that calls Diatheke for each individual verse, which takes over 20 minutes.

[2018-01-12 Fri 11:46]  Well, that works in the 1.8.0 source directory, but the executable it produced only works when called from a shell script in the source directory that is generated by the build process.  I guess if I installed the whole package (libsword and all), the resulting executable would work from elsewhere.

So I tried to make a version for 1.6.2 (the version in Ubuntu Trusty), and it almost works correctly, but the verse text has =[]= in random places.  I don't know if it's because of something I did wrong or changes in the SWORD API.  I had to add some code I found on StackOverflow to trim strings, and I had to convert some of the old API output to =std::string=.

So the problems now are:

+  The 1.8.0 version only runs from within the source directory.
+  The 1.6.2 version has garbage in the output.

If I could build the 1.8.0 version statically, that would probably solve it, but I've no idea how to just build one executable in a project statically.  Might be a simple switch in a command, or it might be much more complex.

I might be able to fix the output in the 1.6.2 version, but since I don't understand exactly what's going on with the buffers and strings and such, I'm not sure what to do.

[2018-01-12 Fri 12:05]  I found [[https://stackoverflow.com/a/15475134][this StackOverflow answer]], and I modified the Makefile in both versions (in the =examples/cmdline= directory) to add those flags to =CXXCOMPILE= and =CXXLINK=, and it made the =examples/cmdline/lookup= file a binary instead of the shell script, but apparently only the =libsword= stuff was statically linked, because =ldd= shows that it's still dynamically linked to a bunch of general shared libraries like =libc=.  But the binary does work when I copy it out of the source directory, so I guess it will do.  This lets me statically link the 1.8.0 version of =libsword=, so I can use it with 1.6.2 still installed on the system.

** [2018-01-13 Sat 04:35]  Time for database?

Now that I have a decent JSON version, maybe it's time to think about SQLite database schemas.  There are two basic possibilities that I can think of:

1.  One table for verses, mapping book IDs to a book table which contains book names and abbreviations.
2.  One table for verses, with book names as strings.

The second one might actually be better because of its simplicity, and with the proper indexing, it should perform fine, AFAIK.

[2018-01-13 Sat 06:33]  It works!  The new =json-to-sqlite.py= script reads the flat-style JSON file (one object per verse, not hierarchical), makes a new SQLite database, and inserts the verses.  I used a Python iterator to make the code fairly simple.

I also wrote a Python script yesterday, =json-verses-to-bible.py=, that converts a flat JSON list of objects to a hierarchical object.  Seems like that could be handy, but it's easier to insert the flat version into the database, and that's what the C++ program makes.

I guess now I'll write a Python script to query the database...

** [2018-01-13 Sat 07:50]  Python is faster than the C++ SWORD API

This is amazing:

#+BEGIN_EXAMPLE
$ time ./query.py query esv.json "Genesis" >/dev/null
0.14user 0.07system 0:00.21elapsed 98%CPU (0avgtext+0avgdata 40204maxresident)k
0inputs+0outputs (0major+10468minor)pagefaults 0swaps

$ time diatheke -b ESV -k "Genesis" >/dev/null
0.37user 0.02system 0:00.41elapsed 98%CPU (0avgtext+0avgdata 23328maxresident)k
0inputs+0outputs (0major+3792minor)pagefaults 0swaps
#+END_EXAMPLE

Printing the entire book of Genesis, parsed from a JSON file containing the entire ESV (minus all the metadata the SWORD formats can contain) is about twice as fast as printing Genesis using Diatheke, which is written in C++ and uses the SWORD C++ libraries.

Now imagine if we /did/ want to store all that metadata, if we stored it separately from the plain-text of the translation.  Whether SQLite or JSON, imagine how much faster it would be!

Oh, and querying the SQLite database is even faster still:

#+BEGIN_EXAMPLE
  $ time ./query.py query esv.sqlite "Genesis" >/dev/null
  0.06user 0.01system 0:00.09elapsed 77%CPU (0avgtext+0avgdata 13172maxresident)k
  8inputs+0outputs (0major+2215minor)pagefaults 0swaps
#+END_EXAMPLE

And I can easily change the output format too:

#+BEGIN_EXAMPLE
  $ time ./query.py query --output json esv.sqlite "Genesis" >/dev/null
  0.08user 0.02system 0:00.11elapsed 99%CPU (0avgtext+0avgdata 16776maxresident)k
  0inputs+0outputs (0major+3103minor)pagefaults 0swaps
#+END_EXAMPLE

Which effectively gives me two-way conversion between formats.

This makes everything so much easier.  Now I should be able to easily access the Bible in Emacs, too.

[2018-01-13 Sat 09:17]  Added full-text search with SQLite's FTS4 engine.  It's so much faster than Diatheke:

#+BEGIN_EXAMPLE
  $ time diatheke -b ESV -s multiword -k jesus >/dev/null
  1.40user 0.16system 0:01.58elapsed 99%CPU (0avgtext+0avgdata 27508maxresident)k
  0inputs+0outputs (0major+5097minor)pagefaults 0swaps

  $ time ./query.py search esv.sqlite "Jesus" >/dev/null
  0.06user 0.00system 0:00.07elapsed 97%CPU (0avgtext+0avgdata 13424maxresident)k
  0inputs+0outputs (0major+2276minor)pagefaults 0swaps
#+END_EXAMPLE

And note that Diatheke /only returns the references, not the actual text of the passages/, while this Python script returns the full text of every matching passage.  In fact, even doing full-text search on the JSON file in Python is much faster than Diatheke:

#+BEGIN_EXAMPLE
  $ time ./query.py search esv.json "Jesus" >/dev/null
  0.12user 0.08system 0:00.21elapsed 99%CPU (0avgtext+0avgdata 40056maxresident)k
  0inputs+0outputs (0major+7608minor)pagefaults 0swaps
#+END_EXAMPLE

* Design

** SQLite Database

*** Features

**** [[https://en.wikipedia.org/wiki/Database_index#Clustered][Clustered index]] / [[https://www.sqlite.org/withoutrowid.html][WITHOUT ROWID]]

*Note:* This is incompatible with the FTS4 engine, so we're not using this after all.

Sounds like I should use this feature.

#+BEGIN_QUOTE
Clustering alters the data block into a certain distinct order to match the index, resulting in the row data being stored in order. Therefore, only one clustered index can be created on a given database table. Clustered indices can greatly increase overall speed of retrieval, but usually only where the data is accessed sequentially in the same or reverse order of the clustered index, or when a range of items is selected.

Since the physical records are in this sort order on disk, the next row item in the sequence is immediately before or after the last one, and so fewer data block reads are required. The primary feature of a clustered index is therefore the ordering of the physical data rows in accordance with the index blocks that point to them. Some databases separate the data and index blocks into separate files, others put two completely different data blocks within the same physical file(s).
#+END_QUOTE

I'm not sure if this means that I need to insert the verses in order (although I intend to, anyway).  But since the book-chapter-verse triplet won't sort in Bible-order (since book names aren't sorted alphabetically), I probably need to.  (I guess I could store book order in a separate table, but that is probably not worth the trouble.)

#+BEGIN_QUOTE
Thus, in some cases, a WITHOUT ROWID table can use about half the amount of disk space and can operate nearly twice as fast. Of course, in a real-world schema, there will typically be secondary indices and/or UNIQUE constraints, and the situation is more complicated. But even then, there can often be space and performance advantages to using WITHOUT ROWID on tables that have non-integer or composite PRIMARY KEYs.
#+END_QUOTE

Sounds like this is definitely what I should use.

#+BEGIN_QUOTE
The WITHOUT ROWID optimization is likely to be helpful for tables that have non-integer or composite (multi-column) PRIMARY KEYs and that do not store large strings or BLOBs.
#+END_QUOTE

I'm not sure what counts as a "large string," but I'm guessing that individual verses here aren't large enough to.

#+BEGIN_QUOTE
WITHOUT ROWID tables work best when individual rows are not too large. A good rule-of-thumb is that the average size of a single row in a WITHOUT ROWID table should be less than about 1/20th the size of a database page. That means that rows should not contain more than about 50 bytes each for a 1KiB page size or about 200 bytes each for 4KiB page size. WITHOUT ROWID tables will work (in the sense that they get the correct answer) for arbitrarily large rows - up to 2GB in size - but traditional rowid tables tend to work faster for large row sizes. This is because rowid tables are implemented as B*-Trees where all content is stored in the leaves of the tree, whereas WITHOUT ROWID tables are implemented using ordinary B-Trees with content stored on both leaves and intermediate nodes. Storing content in intermediate nodes mean that each intermediate node entry takes up more space on the page and thus reduces the fan-out, increasing the search cost.
#+END_QUOTE

For most verses that probably does apply (although I don't know what the page size will be, or if I should set it manually).  Most verses are probably less than 50 bytes, and probably few, if any, are 200 bytes (although I don't know if that should include the book name).

#+BEGIN_QUOTE
Note that except for a few corner-case differences detailed above, WITHOUT ROWID tables and rowid tables work the same. They both generate the same answers given the same SQL statements. So it is a simple matter to run experiments on an application, late in the development cycle, to test whether or not the use of WITHOUT ROWID tables will be helpful. A good strategy is to simply not worry about WITHOUT ROWID until near the end of product development, then go back and run tests to see if adding WITHOUT ROWID to tables with non-integer PRIMARY KEYs helps or hurts performance, and retaining the WITHOUT ROWID only in those cases where it helps.
#+END_QUOTE

Ok, so it's not a big deal, and I can easily change it later.

**** [[https://www.sqlite.org/datatype3.html#collation][Collation]]

#+BEGIN_QUOTE
When SQLite compares two strings, it uses a collating sequence or collating function (two words for the same thing) to determine which string is greater or if the two strings are equal. SQLite has three built-in collating functions: BINARY, NOCASE, and RTRIM.

BINARY - Compares string data using memcmp(), regardless of text encoding.
NOCASE - The same as binary, except the 26 upper case characters of ASCII are folded to their lower case equivalents before the comparison is performed. Note that only ASCII characters are case folded. SQLite does not attempt to do full UTF case folding due to the size of the tables required.
RTRIM - The same as binary, except that trailing space characters are ignored.
#+END_QUOTE

I guess I should probably use =NOCASE=, but for matching book names I will probably want something more advanced.

**** Primary key

*Note:* Also not relevant for FTS4.

#+BEGIN_QUOTE
Each table in SQLite may have at most one PRIMARY KEY. If the keywords PRIMARY KEY are added to a column definition, then the primary key for the table consists of that single column. Or, if a PRIMARY KEY clause is specified as a table-constraint, then the primary key of the table consists of the list of columns specified as part of the PRIMARY KEY clause. The PRIMARY KEY clause must contain only column names — the use of expressions in an indexed-column of a PRIMARY KEY is not supported. An error is raised if more than one PRIMARY KEY clause appears in a CREATE TABLE statement. The PRIMARY KEY is optional for ordinary tables but is required for WITHOUT ROWID tables.
#+END_QUOTE

I guess I should make it on the =book=, =chapter=, and =verse= columns.

**** UNDERWAY Full-text search
:LOGBOOK:
-  State "UNDERWAY"   from "DONE"       [2018-01-13 Sat 11:03]
:END:

Works great with SQLite's FTS4 engine.

***** TODO Query syntax
:LOGBOOK:
-  State "TODO"       from              [2018-01-13 Sat 11:03]
:END:

Note this from the FTS docs:

#+BEGIN_QUOTE
The FTS modules may be compiled to use one of two slightly different versions of the full-text query syntax, the "standard" query syntax and the "enhanced" query syntax. The basic term, term-prefix, phrase and NEAR queries described above are the same in both versions of the syntax. The way in which set operations are specified is slightly different. The following two sub-sections describe the part of the two query syntaxes that pertains to set operations. Refer to the description of how to compile fts for compilation notes.
#+END_QUOTE

And one of the key differences is:

#+BEGIN_QUOTE
FTS query set operations using the standard query syntax are similar, but not identical, to set operations with the enhanced query syntax. There are four differences, as follows:

1.  Only the implicit version of the AND operator is supported. Specifying the string "AND" as part of a standard query syntax query is interpreted as a term query for the set of documents containing the term "and".
#+END_QUOTE

Apparently my SQLite uses the standard syntax, because when I search for =Jesus AND Paul AND apostle AND called=, I only get one result:

#+BEGIN_EXAMPLE
  1 Corinthians 1:1: Paul, called by the will of God to be an apostle of Christ Jesus, and our brother Sosthenes,
#+END_EXAMPLE

While if I search for =Jesus Paul apostle called= I get:

#+BEGIN_EXAMPLE
  Romans 1:1: Paul, a servant of Christ Jesus, called to be an apostle, set apart for the gospel of God,
  I Corinthians 1:1: Paul, called by the will of God to be an apostle of Christ Jesus, and our brother Sosthenes,
#+END_EXAMPLE

So the =AND= is interpreted literally, and Ro 1:1 is not returned because it doesn't have =and= in it.

I can't control this, because it depends on how the local SQLite copy is compiled, so I'll have to document it in the readme.

**** Unique constraint

#+BEGIN_QUOTE
A UNIQUE constraint is similar to a PRIMARY KEY constraint, except that a single table may have any number of UNIQUE constraints. For each UNIQUE constraint on the table, each row must contain a unique combination of values in the columns identified by the UNIQUE constraint. For the purposes of UNIQUE constraints, NULL values are considered distinct from all other values, including other NULLs. As with PRIMARY KEYs, a UNIQUE table-constraint clause must contain only column names — the use of expressions in an indexed-column of a UNIQUE table-constraint is not supported.
#+END_QUOTE

So I do /not/ want a unique constraint on the =text= column, because even though it's unlikely that two verses in the Bible would be exactly the same, I don't know if it's actually the case, and it's theoretically possible.

*** Schema

**** Primary key

**** Tables

***** Verses

#+BEGIN_SRC sql
  CREATE TABLE verses (
  book TEXT PRIMARY KEY,
  chapter INTEGER PRIMARY KEY,
  verse INTEGER PRIMARY KEY,
  text TEXT)
  WITHOUT ROWID;
#+END_SRC

FTS4 version:

#+BEGIN_SRC sql
  CREATE VIRTUAL TABLE verses USING fts4 (
  book TEXT,
  chapter INTEGER,
  verse INTEGER,
  text TEXT);
#+END_SRC

***** Book abbreviations

Might be a good idea to have this, but we'll probably still need some client-level heuristics to handle book-choosing.

***** Book numbers

Might be a good idea to have a table that maps book names to book numbers.  Across translations or (or...what is the right word?  codex?  anthology?  work?) types of Bibles, this might vary, but within a database, for a single translation, it would likely be useful.
